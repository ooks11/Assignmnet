{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c64232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2193fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40079 images belonging to 10 classes.\n",
      "Found 9921 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"cifar-10-img/train\"\n",
    "test_dir = \"cifar-10-img/test\"\n",
    "\n",
    "image_data_generator = ImageDataGenerator(rescale=1.0/255)\n",
    "train_batch_size = 5000\n",
    "test_batch_size = 1000\n",
    "\n",
    "train_generator = image_data_generator.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=train_batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = image_data_generator.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(32, 32),\n",
    "    batch_size=test_batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b78be9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train =  train_generator[0]\n",
    "x_test, y_test = test_generator[0]\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2f3b2f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=./vgg16_weights_tf_dim_ordering_tf_kernels_notop (1).h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load VGG16 without top layers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./vgg16_weights_tf_dim_ordering_tf_kernels_notop (1).h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m base_model \u001b[38;5;241m=\u001b[39m VGG16(weights\u001b[38;5;241m=\u001b[39mweights_path, include_top\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/applications/vgg16.py:96\u001b[0m, in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, classifier_activation, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Instantiates the VGG16 model.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mReference:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    A `Model` instance.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (weights \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m} \u001b[38;5;129;01mor\u001b[39;00m file_utils\u001b[38;5;241m.\u001b[39mexists(weights)):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `weights` argument should be either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`None` (random initialization), \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(pre-training on ImageNet), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor the path to the weights file to be loaded.  Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     )\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m include_top \u001b[38;5;129;01mand\u001b[39;00m classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using `weights=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` with `include_top=True`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`classes` should be 1000.  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived classes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclasses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The `weights` argument should be either `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded.  Received: weights=./vgg16_weights_tf_dim_ordering_tf_kernels_notop (1).h5"
     ]
    }
   ],
   "source": [
    "# Load VGG16 without top layers\n",
    "weights_path = \"./vgg16_weights_tf_dim_ordering_tf_kernels_notop (1).h5\"\n",
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb43afdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m base_model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m      2\u001b[0m    layer\u001b[38;5;241m.\u001b[39mtrainable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "   layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11cf85d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32109cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 795ms/step - accuracy: 0.2226 - loss: 2.1457 - val_accuracy: 0.4090 - val_loss: 1.6241\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 711ms/step - accuracy: 0.4098 - loss: 1.6112 - val_accuracy: 0.4740 - val_loss: 1.4448\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 683ms/step - accuracy: 0.4658 - loss: 1.5027 - val_accuracy: 0.5160 - val_loss: 1.3576\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 683ms/step - accuracy: 0.5281 - loss: 1.3454 - val_accuracy: 0.5170 - val_loss: 1.3329\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 778ms/step - accuracy: 0.5320 - loss: 1.3037 - val_accuracy: 0.5220 - val_loss: 1.2830\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 704ms/step - accuracy: 0.5609 - loss: 1.2159 - val_accuracy: 0.5400 - val_loss: 1.3019\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 883ms/step - accuracy: 0.5635 - loss: 1.2021 - val_accuracy: 0.5290 - val_loss: 1.2778\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 772ms/step - accuracy: 0.5979 - loss: 1.1450 - val_accuracy: 0.5530 - val_loss: 1.2352\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 750ms/step - accuracy: 0.6153 - loss: 1.0814 - val_accuracy: 0.5480 - val_loss: 1.2424\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 702ms/step - accuracy: 0.6183 - loss: 1.0676 - val_accuracy: 0.5440 - val_loss: 1.2393\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x209c6c46790>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0f14ff38",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights=weights_path, include_top=False, input_shape=(32, 32, 3))\n",
    "# freeze all layers first\n",
    "for layer in base_model.layers:\n",
    "   layer.trainable = False\n",
    "# unfreeze last 4 layers of base model\n",
    "for layer in base_model.layers[len(base_model.layers) - 4:]:\n",
    "   layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca7ce72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - accuracy: 0.2176 - loss: 2.1002 - val_accuracy: 0.4850 - val_loss: 1.4182\n",
      "Epoch 2/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - accuracy: 0.4921 - loss: 1.3615 - val_accuracy: 0.5460 - val_loss: 1.2815\n",
      "Epoch 3/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 1s/step - accuracy: 0.6140 - loss: 1.1054 - val_accuracy: 0.5940 - val_loss: 1.1327\n",
      "Epoch 4/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2s/step - accuracy: 0.6686 - loss: 0.9621 - val_accuracy: 0.6170 - val_loss: 1.1588\n",
      "Epoch 5/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 2s/step - accuracy: 0.7247 - loss: 0.7867 - val_accuracy: 0.6210 - val_loss: 1.2368\n",
      "Epoch 6/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 1s/step - accuracy: 0.7736 - loss: 0.6838 - val_accuracy: 0.6220 - val_loss: 1.2089\n",
      "Epoch 7/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.7786 - loss: 0.6200 - val_accuracy: 0.6150 - val_loss: 1.2936\n",
      "Epoch 8/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 2s/step - accuracy: 0.8297 - loss: 0.5107 - val_accuracy: 0.6450 - val_loss: 1.3941\n",
      "Epoch 9/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 2s/step - accuracy: 0.8495 - loss: 0.4545 - val_accuracy: 0.6160 - val_loss: 1.4157\n",
      "Epoch 10/10\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - accuracy: 0.8551 - loss: 0.4265 - val_accuracy: 0.6620 - val_loss: 1.2831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x209d4494910>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tuning hyper parameters\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# training fine tuned model\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc2a1610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 361ms/step\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predicted_value = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17357468",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(test_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a571664d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preditcted:  ship\n",
      "Actual:  ship\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 890\n",
    "plt.imshow(x_test[n])\n",
    "print(\"Preditcted: \",labels[np.argmax(predicted_value[n])])\n",
    "print(\"Actual: \", labels[np.argmax(y_test[n])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a76f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
